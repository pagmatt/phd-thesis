%\documentclass[draftclsnofoot, onecolumn, 12pt]{IEEEtran}

% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
% \usepackage{cite}
% \usepackage{amsmath, mathtools}
% \usepackage{amsfonts,amsthm,bm}
% \usepackage{amssymb}
% \usepackage{comment}
% \usepackage{graphicx}
% \usepackage{standalone}
% \usepackage{dsfont}
% \usepackage{mathtools}
% \usepackage{color, colortbl}
% \usepackage{soul}
% \usepackage[normalem]{ulem}
% \usepackage[acronym,shortcuts]{glossaries}
% \usepackage{arydshln}
% \usepackage{bbm}
% \usepackage{svg} 
% \usepackage{algorithm,algorithmic}
% \usepackage{adjustbox}
% \usepackage[inline]{enumitem}
% \usepackage{multirow}
% \usepackage{tabularx}
% \usepackage{booktabs}
% \newcommand{\tabitem}{~~\llap{\textbullet}~~}
% \usepackage{comment}
% \usepackage{tikz}
% \usepackage{pgfplots}
% \pgfplotsset{compat=newest}
% \pgfplotsset{plot coordinates/math parser=false}
% \newlength\fheight
% \newlength\fwidth
% \usetikzlibrary{plotmarks,patterns,decorations.pathreplacing,backgrounds,calc,arrows,arrows.meta,spy,matrix}
% \usepgfplotslibrary{patchplots,groupplots}
% \usepackage{tikzscale}

%%%% FOR MULTIPLE BIBLIOGRAPHIES %%%%%%
% \usepackage[resetlabels,labeled]{multibib}
% \newcites{A}{Additional References}

% % Patterns
% \usetikzlibrary {patterns.meta}

% \newtheorem{theorem}{Theorem}

% \newcommand\blfootnote[1]{%
%   \begingroup
%   \renewcommand\thefootnote{}\footnote{#1}%
%   \addtocounter{footnote}{-1}%
%   \endgroup
% }

% \usepackage{ulem}

% \providecolor{added}{rgb}{0,0,1}
% \providecolor{deleted}{rgb}{1,0,0}
%% Change tracking with ulem
% \newcommand{\ccadd}[1]{{\color{added}{}#1}}
% \newcommand{\ccdel}[1]{{\color{deleted}\sout{#1}}}


%acronyms
\newacronym{cwc}{CWC}{capacity-weighted clustering}
\newacronym{dlos}{dLoS}{deterministic \gls{los}}
\newacronym{fov}{FoV}{field-of-view}
\newacronym{hc}{HC}{hierarchical clustering}
\newacronym{km}{KM}{K-means}
\newacronym{kmed}{KMed}{K-medoids}
\newacronym{icwc}{ICWC}{inverse capacity-weighted clustering}
\newacronym{isd}{ISD}{inter-site distance}
\newacronym{lsfc}{LSFC}{large-scale fading coefficient}
\newacronym{minlp}{MINLP}{mixed integer nonlinear programming}
\newacronym{noma}{NOMA}{non-orthogonal multiple access}
\newacronym{oscbc}{OSCBC}{one-shot capacity-based clustering}
\newacronym{ofdma}{OFDMA}{orthogonal frequency-division multiple access}
\newacronym{oma}{OMA}{orthogonal multiple access}
\newacronym{pam}{PAM}{partition around medoids}
\newacronym{plos}{pLoS}{probabilistic \gls{los}}
\newacronym{rb}{RB}{resource block}
\newacronym{rsma}{RSMA}{rate-splitting multiple access}
\newacronym{ula}{ULA}{uniform linear array}

\section{Introduction}
\label{sec:introduction}

The ever-increasing growth of mobile traffic has called both academia and industry to identify and develop solutions for extending the radio spectrum beyond the crowded sub-6 GHz bands. As a result, the use of \gls{mmwave} band for cellular communications has been included in the latest releases of the \gls{3gpp}  standard, namely \gls{5g} New Radio (NR)~\cite{3gpp.38.104}. Moreover, \gls{thz} frequencies are being investigated as enablers for \gls{6g} networks as well~\cite{tariq2020speculative}.

However, transmissions in the \gls{mmwave} and \gls{thz} bands are subject to challenging propagation conditions, mainly due to severe path loss and susceptibility to blockages~\cite{rangan2017potentials}.
To mitigate these limitations, the research community has explored solutions to improve network coverage, for example using \gls{iab} nodes, as also approved by the \gls{3gpp} as part of \gls{5g} \gls{nr} specifications for Rel-16~\cite{3gpp.38.174}.
In particular, \gls{iab} allows base stations, or \glspl{gnb} in 5G NR parlance, to establish wireless (rather than traditional fiber-like) backhaul links, possibly through multiple hops,  to a donor, thus reducing deployment costs~\cite{polese2020integrated}. Still, \gls{iab} involves complex signal processing and saturation of the available resources and may be costly and energy-consuming for network operators.

In light of this, \glspl{irs} are being investigated as solutions to overcome the harsh propagation conditions shown by \gls{mmwave} and \gls{thz} bands in a cost- and energy-efficient manner~\cite{flamini2022towards}. 
\glspl{irs} are meta-surfaces, whose radiating elements can \emph{passively} tune the phase shift of impinging signals to favorably alter an electromagnetic field towards an intended destination. They can be configured to beamform the reflected signal virtually in any direction, hence acting as a relay to improve the signal quality without an active (power-consuming) amplification~\cite{bjornson2019intelligent}. 

\subsection{Prior Work}
Despite the substantial research hype, most recent studies on \glspl{irs} rely on strong assumptions that do not match real-world deployments.
Specifically, a significant body of literature is based on the assumption that \glspl{irs} establish an ideal (i.e., fiber-like) control channel with the base station~\cite{wu2020towards, abeywickrama2020intelligent, wu2019intelligent, pagin2022end}. Instead, actual deployments are expected to feature a wireless, i.e., error-prone, \gls{irs} control, possibly implemented with low-cost technologies~\cite{liu2022path, liaskos2018realizing}. 
This introduces constraints on the \gls{irs} reconfiguration period, which needs to be synchronized with the base station to beamform the signal towards the \gls{ue} served during the specific time slot~\cite{flamini2022towards}, a similar research problem to scheduling in cellular networks. 

In this perspective, \gls{irs}-assisted downlink scheduling solutions have been widely studied in different domains, each with its own theoretical constraints.
For example, in \gls{ofdma} user scheduling, all the users scheduled in a given time slot must be served using the same reflection coefficients, due to the lack of frequency selective beamforming capabilities at the \gls{irs}. In this context, dynamic optimization schemes, wherein the \gls{irs} configurations are adjusted at each time slot, have been studied in ~\cite{Yang20IRS, Lee23Harmony}.
The authors of \cite{guo2021intelligent} consider a two-user downlink transmission problem in an \gls{irs}-assisted scenario over fading channels, and compare the results of different basic \gls{oma} and \gls{noma} schemes. It is found that, while \gls{noma} is the best solution, by exploiting \gls{irs} reconfiguration in each slot of the fading block, \gls{tdma} outperforms \gls{fdma}, and its performance is similar to that of \gls{noma}.
A hybrid \gls{tdma}-\gls{noma} approach, instead, was investigated in
an uplink scenario in \cite{zhang2021throughput, alobiedollah2023self}, in the context of a wireless-powered network, where users are grouped based on their channel gains. 
Then, \glspl{ue} within the same group transmit in a non-orthogonal fashion, while different groups are assigned to different time slots.
Moreover, a user scheduling algorithm based on graph neural networks, able to jointly optimize the \gls{irs} configuration and the \gls{gnb} beamforming in downlink, was recently presented in~\cite{Zhang22Learning}. 
Similarly, the authors of~\cite{Bansal21Rate, Fu21Resource, Zhuo22Partial} evaluated the performance of several non-orthogonal downlink scheduling methods, such as \gls{rsma}.
Finally, \glspl{irs} with energy harvesting capabilities are considered in~\cite{hu2021robust}. In this work, the authors propose a trade-off between the system sum capacity and the \gls{irs} energetic self-sustainability, with the goal of achieving coverage flexibility and low deployment costs.

Still, most of the literature poses little to no reconfiguration constraints for the IRS.
However, early \gls{irs} control circuitry prototypes, which have low power consumption (i.e., a few hundreds of mW), have a non-negligible phase-shifts reconfiguration time \cite{mu2021capacity, ETSIGRRIS003}, thus posing additional constraints in the system design. For example, the prototypes in \cite{rossanese2022designing} and \cite{alexandropoulos2023ris} have a reconfiguration time of a few tens of ms, even though architectures based on \gls{fpga} such as in \cite{yezhen2020novel} promise to achieve much lower configuration times, i.e., in the order of tens of microseconds.
Still, the overhead (in terms of time)  increases as the number of IRS elements increases, as investigated in \cite{jamali2022low, nadeem2020asymptotic, Qian22joint}. 
In any case, a constraint on the number of reconfigurations (and relative period) is desirable to ensure system synchronization and minimize the IRS downtime during reconfiguration. In this regard, it is of interest to 
\begin{enumerate*}[label=(\textit{\roman*})]
  \item investigate the level of performance degradation experienced by \gls{irs}-assisted systems when considering practical constraints, including limitations in the number of reconfigurations, and
  \item design algorithms that can mitigate these constraints. 
\end{enumerate*}
The limitation on the number of \gls{irs} reconfigurations in a given time frame has been initially studied in \cite{mu2021capacity}, where the authors evaluate the capacity of both \gls{oma} and \gls{noma} schemes of a 2-user \gls{irs}-assisted \gls{siso} system under Rayleigh fading conditions. Still, additional research efforts is required to fully characterize the impact of IRS reconfigurations constraints on the network.

\subsection{Contributions}
\begin{comment}
In this paper, we propose a \gls{tdma} scheduling policy for downlink cellular transmissions based on clustering algorithms, to maximize the sum capacity in \gls{irs}-assisted network deployments with practical constraints.
Notably, we assume a fixed maximum number of \gls{irs} reconfigurations within a time frame and aim at optimizing both the reconfiguration time and the resulting \gls{irs} configuration(s).
The limit on the number of reconfigurations sets a simple constraint on the overhead entailed by the control of the \gls{irs}. 
Our contributions can be summarized as follows.
First, we formalize an optimization problem to determine the optimal \gls{irs} configuration(s) to maximize the sum capacity. Specifically, we partition the \glspl{ue} into disjoint groups, and propose an algorithm that iteratively optimizes the \gls{irs} configuration for each group, as well as the relative beamformers to be used for transmission.
Secondly, we convert the sum capacity problem into a clustering problem to determine the optimal set of groups, or clusters, for the \glspl{ue} based on their channel characteristics. 
Accordingly, we design clustering algorithms for the \glspl{ue}, such that all \glspl{ue} in the same cluster will adopt the same \gls{irs} configuration. The goal is to minimize the capacity loss associated with a suboptimal \gls{irs} configuration for each \gls{ue} in the cluster, for promoting communication efficiency and reducing the overhead.
We investigate two clustering techniques: 
\begin{enumerate*}[label=(\textit{\roman*})]
\item distance-based clustering, which adjusts the cluster configuration based on the distance with the optimal \gls{irs} configuration of each \gls{ue} in the cluster;
\item capacity-based clustering, which adjusts the cluster configuration to maximize the sum capacity and/or the user fairness.
 Specifically, we propose three clustering algorithms. The first, named \gls{cwc}, iteratively adjusts the clusters' centroids weighting the points in each cluster by the capacity achieved by each \gls{ue} in the cluster, therefore favoring the best users and maximizing the sum capacity. The second, named \gls{oscbc}, is a low-complexity approach where the centroids are simply the optimal configurations of the \glspl{ue} experiencing the highest \gls{snr} in each cluster, without considering the impact of the remaining \glspl{ue}.  Finally, \gls{icwc} promotes fairness among the \glspl{ue} by weighting the points in the clusters by the inverse of the \glspl{ue}' achievable rate.
 \end{enumerate*}

Moreover, we compare via simulation the performance of the distance- and capacity-based clustering in different \gls{irs}-assisted scenarios.
Extensive numerical results show that there exists a trade-off between the achievable sum capacity and fairness, even though capacity-based clustering techniques can guarantee satisfactory performance, despite some reconfiguration constraints. Also, we demonstrate that scheduling based on clustering can reduce by 50\% the number of \gls{irs} reconfigurations, thus promoting communication efficiency, in view of minor degradation in terms of sum capacity. 
\end{comment}
 % MP: Trimmed
In this paper, we propose a \gls{tdma} scheduling policy for downlink cellular transmissions based on clustering algorithms, to maximize the sum capacity in \gls{irs}-assisted network deployments with practical constraints.
Our main contributions are summarized as follows:
\begin{itemize}%[label=(\textit{\roman*})]
    \item We account for practical \gls{irs} limitations by considering a fixed maximum number of reconfigurations of IRS reflecting elements within a time frame, thus setting a simple constraint on the overhead entailed by the control of the \gls{irs}.
    \item We formalize an optimization problem to determine the optimal \gls{irs} configurations to maximize the sum capacity while satisfying the reconfiguration per frame constraint. Then, we convert the sum capacity problem into a clustering problem. The latter determines sets of \glspl{ue} that can be served with the same (possibly suboptimal) \gls{irs} configuration while minimizing the related capacity loss. %associated with a suboptimal \gls{irs} configuration for each \gls{ue} in the cluster, for promoting communication efficiency and reducing the overhead.
    \item We design, as an alternative to typical clustering algorithms based on distance measures, a new class of algorithms which we denote as \textit{capacity-based clustering}. These algorithms adjust the cluster configuration taking into account the sum capacity and the user fairness.
     Specifically, we propose three clustering algorithms: \gls{cwc}, which favors users experiencing the best channel conditions, %and maximizes the sum capacity, 
     \gls{oscbc}, which represents a low-complexity alternative to the former, and \gls{icwc}, which promotes fairness among the cluster \glspl{ue}.
    \item We compare via simulation the performance of distance- and capacity-based clustering in different \gls{irs}-assisted scenarios.
    Extensive numerical results show that scheduling based on clustering can reduce by up to 50\% the number of \gls{irs} reconfigurations, thus promoting communication efficiency at the expense of a slightly lower sum capacity. 
\end{itemize}
With respect to \cite{rech2023downlink}, we introduce new capacity-based clustering strategies to improve fairness and provide more extensive numerical results to demonstrate the scalability of the proposed solutions as a function of the density of \glspl{ue} and the \gls{irs} size. Moreover, we evaluate the performance of the proposed scheduling strategies considering realistic \gls{irs} network constraints, including the quantization of phase shifts, and for different channel propagation conditions.
In this sense, we provide additional results in terms of the computational complexity of the proposed distance- and capacity-based clustering algorithms, as well as in terms of fairness.
Finally, we remark that due to the novelty of the considered scenario, 
to the best of our knowledge the effectiveness of our solution cannot be directly compared with any works in the literature. Indeed, the most similar works~\cite{Lee23Harmony,guo2021intelligent, mu2021capacity,jamali2022low} exhibit substantial differences in the considered contexts.
More specifically:
\begin{itemize}
\item In comparison to \cite{Lee23Harmony}, which assumes the \gls{irs} configurations to be fixed, %where multiple \glspl{irs} are considered and the \gls{irs} configurations are fixed for the entire resource grid, 
our work focuses on optimizing user scheduling in conjunction with \gls{irs} configurations in a dynamic manner. %The f in \cite{Lee23Harmony} makes it incompatible for direct comparison with our dynamic optimization approach.
\item \cite{guo2021intelligent, mu2021capacity} analyze a basic scenario where a single-antenna \gls{gnb} serves single-antenna \glspl{ue}, while we consider multiple antennas at both the base station and \glspl{ue}, and an \gls{ofdma} multiuser access scheme. 
\item %With respect to \cite{mu2021capacity}, the authors consider a scenario with single-antenna \glspl{gnb} and \glspl{ue}. 
%Moreover, their \gls{irs} model differs significantly from ours, as the \gls{irs} panel is divided into equal-size sub-surfaces, which are configured individually. 
While \cite{mu2021capacity} characterizes the capacity region for %both \gls{oma} and \gls{noma} strategies with 
$K>1$ \glspl{ue}, the computational complexity of the proposed scheme limits its applicability to $K=2$ \glspl{ue}. In contrast, our solution supports more realistic scenarios,  where $K \gg 1$.
%\item In \cite{jamali2022low}, the \gls{irs} optimization is performed to constantly guarantee a minimum \gls{snr} requirement at a single \gls{ue} moving with constant speed within the blockage area of the cell. This method, which is based on the \gls{ue} positioning and mobility model, reduces the overhead required for \gls{irs} reconfiguration in the single-\gls{ue} context. Our work, on the other hand, optimizes \gls{irs} configurations for data transmissions to subsets of \glspl{ue} in a multi-user scenario.
\item The authors of \cite{jamali2022low} use the position estimate of a moving \gls{ue} to minimize the \gls{irs} reconfiguration overhead while guaranteeing a minimum \gls{snr} in the single-\gls{ue} context. In our work, on the other hand, 
we consider a multi-user scenario and reduce the number of \gls{irs} configurations by clustering \glspl{ue} in the \gls{csi} domain.
\end{itemize}
These fundamental differences in system setup and assumptions prevent a meaningful simulation-based comparison.

\subsection{Organization and Notation}
The rest of the paper is organized as follows. In Section \ref{sec:system_model}, we introduce the system model. In Section~\ref{sec:sumcapoptimization}, we present the sum capacity optimization problem. In Section \ref{sec:optimization}, we describe the scheduling framework, while in Sections \ref{sec:dist_based} and \ref{sec:cap_based} we present distance-based and capacity-based clustering algorithms, respectively. In Section~\ref{sec:numerical_results}, we show numerical results and compare the different scheduling and clustering solutions. Finally, Section~\ref{sec:conclusions} draws the main conclusions.

Scalars are denoted by italic letters; vectors and matrices by boldface lowercase and uppercase letters, respectively; sets are denoted by calligraphic uppercase letters.
$\diag(\bm{a})$ indicates a square diagonal matrix with the elements of $\bm{a}$ on the principal diagonal, and $\vect(\bm{A})$ denotes the vectorization operator, staking the columns of matrix $\bm{A}$ into a column vector. 
$\bm{A}^{\mathrm T}$ and $\bm{A}^\dagger$ denote the transpose and the conjugate transpose of matrix $\bm{A}$, respectively. 
$[\bm{A}]_{k \ell}$ denotes the scalar value in the $k$-th row and $\ell$-th column of matrix $\bm{A}$, while $[\bm{a}]_k$ denotes the $k$-th element of vector $\bm{a}$. 
The imaginary unit is denoted as $j =\sqrt{-1}$, and $\angle a$ denotes the phase of $a \in \mathbb{C}$. 
The operator $\diamond$ denotes the Khatri-Rao product.
Finally, $\mathbb{E}[\cdot]$ denotes statistical expectation.


\section{System Model}
\label{sec:system_model}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth, trim={2.8cm 0 1.8cm 0},clip]{Figures/IrsClustering/journal_sysmod.pdf}
    %\vspace{-25pt}
    \caption{Downlink \gls{tdma} scheduling for multi-user \gls{irs}-aided systems.}
    %\vspace{-15pt}
    \label{fig:system_model}
\end{figure}

We consider downlink data transmissions for the multi-user \gls{mimo} communication system shown in Fig.~\ref{fig:system_model}, wherein the transmission from the \gls{gnb} to the $K$ \glspl{ue} is assisted by an \gls{irs}.
The \gls{gnb} and the \glspl{ue} are equipped with $N_{\mathrm g}$ and $N_{\mathrm U}$ antennas, respectively. We assume that the direct link between the \gls{gnb} and the \glspl{ue} is unavailable due to blockage. As a consequence, the \gls{gnb} transmits signals to the \glspl{ue} by exploiting the virtual link offered by the \gls{irs}. In this context, the \gls{irs} configuration is managed by the \gls{gnb} through the \gls{irs} controller, by exploiting a dedicated link between the \gls{gnb} and the \gls{irs}, thus with no additional communication overhead in the \gls{gnb}-\gls{ue} link.
Time is divided into frames of $K$ slots, and each \gls{ue} is served exactly once in a frame in a \gls{tdma} fashion, which ensures there is no co-channel interference as \glspl{ue} are separated in the time domain. In our scenario we expect that the \gls{gnb}-\gls{irs} channel has rank one, i.e., a single dominant path, which effectively prevents multi-stream transmissions and spatial multiplexing. 
However, when higher-rank channels are available, either multi-stream transmission to each \gls{ue} or spatial multiplexing can be considered. For the former case, the proposed solution applies straightforwardly. 
Moreover, our approach can to be suitably modified to accommodate for the latter scenario. A detailed investigation of this point is left for future work.
We assume that \glspl{ue} are either static or moving slowly, which is the most typical application scenario for \gls{irs}-assisted networks. Under such conditions, the channel coherence time is in the order of 10~ms~\cite[Fig.~5]{lu2020positioning}. Considering that perfect \gls{csi} of all \glspl{ue} is acquired at the \gls{gnb} at the beginning of each frame (a realistic assumption that does not affect the proposed scheduling framework for IRS communication), it is reasonable to conclude that the channel remains constant throughout the whole time frame. Here, we assume that the CSI is available for any IRS configuration.

\subsection{IRS Model}\label{sec:irsmodel}
Each of the $N_{\mathrm I}$ elements of the \gls{irs} acts independently as an omnidirectional antenna unit that reflects the impinging electromagnetic field by introducing a tunable phase shift on the baseband-equivalent signal. We denote as $\phi_n=e^{j\theta_n}$ the reflection coefficient of the $n$-th \gls{irs} element, where $\theta_n \in \mathcal{P}_{\theta}$ is the induced phase shift, and $\mathcal{P}_{\theta}$ is the set of possible phase shifts. 
Recent works argue that continuous phase shifts are hardly implementable in practice \cite{Tan2018}. Therefore, we consider both continuous and quantized phase shifts. While in the former case the set of phase shifts is $\mathcal{P}_{\theta}= [-\pi, \pi)$, in the latter we have 
$\mathcal{P}_{\theta} = \left\{0, \frac{2\pi}{2^b},\ldots,\frac{2\pi(2^b-1)}{2^b} \right\}$ where $b>0$ is the number of bits employed to quantize the phase shifts. 

We denote with $\bm{H} \in \mathbb{C}^{N_{\mathrm I} \times N_{\mathrm g}}$ the channel matrix between the \gls{irs} and the gNB, and with $\bm{G}_k \in \mathbb{C}^{N_{\mathrm U} \times N_{\mathrm I}}$ the channel matrix of the link between the \gls{irs} and \gls{ue} $k$, respectively.
We consider single-stream transmissions,\footnote{The assumption of single-stream transmissions is justified by the rank of the cascade channel matrix, which is likely equal to one. This conclusion comes from the considerations reported in \cite{he2020cascaded, rains2023ris,rappaport2017investigation}, and has been verified numerically for the considered setup.} with $\bm{w}_k \in \mathbb{C}^{N_{\mathrm g}\times 1}$ and $\bm{v}_k\in \mathbb{C}^{N_{\mathrm U}\times 1}$ defined as the beamforming vectors at the \gls{gnb} and \gls{ue} $k$, respectively.
Let $x_k$ be the single-stream signal transmitted by the \gls{gnb} to \gls{ue} $k$; the received post-processing signal can be expressed as
\begin{equation}
    z_k = \bm{v}_k^{\mathrm T} \bm{G}_k \bm{\Phi} \bm{H}  \bm{w}_kx_k + \bm{v}_k^{\mathrm T} \bm{n}_{k},
\end{equation}
where $\bm{n}_k\in \mathbb{C}^{N_{\mathrm U}\times 1}$ represents the circularly symmetric complex Gaussian noise vector with entries having zero mean and variance $\sigma^2_{n}$, while $\bm{\Phi} \in \mathbb{C}^{N_{\mathrm I}\times N_{\mathrm I}}$ is the {\gls{irs} configuration}, i.e., a diagonal matrix defined as $\bm{\Phi} = \diag(\phi_1,\ldots,\phi_{N_{\mathrm I}})$.
Note that different, and specific, \gls{irs} configurations can be adopted for different \glspl{ue}. Accordingly, in the rest of the paper we let $\bm{\Phi}_k$ be the \gls{irs} configuration adopted when \gls{ue} $k$ is served.

The \gls{snr} at \gls{ue} $k$ under \gls{irs} configuration $\bm{\Phi}_k$ is
\begin{equation}\label{snr_k}
    \Gamma_k(\bm{\Phi}_k) = \frac{|\bm{v}_k^{\mathrm T} \bm{G}_k \bm{\Phi}_k \bm{H}  \bm{w}_k |^2\sigma_{x}^2}{|\bm{v}_k|^2\sigma_{n}^2},
\end{equation}
where $\sigma_{x}^2$ is the power of the transmitted signal.
To maximize the \gls{snr} of a given \gls{ue}, a specific \gls{irs} configuration should be adopted, tailored to the \gls{ue} position in the cell and the channel conditions.
However, the goal of this paper is to limit the number of \gls{irs} reconfigurations to comply with realistic overhead constraints, as well as to improve the communication efficiency, and algorithms seeking to comply with these requirements will be presented in Section~\ref{sec:optimization}.

\section{Sum Capacity Optimization Problem}\label{sec:sumcapoptimization}

We impose a constraint on the number of \gls{irs} reconfigurations per time frame, with the goal of either limiting the reconfiguration,\footnote{We remark that the \gls{gnb} typically communicates a (possibly new) \gls{irs} configuration in each \gls{tti}. The reconfiguration constraint introduced in the proposed IRS scheduling framework is able to reduce this overhead by a factor ${Z}/{K} \leq 1$.} or accounting for practical limitations that might arise in realistic deployments.
On the downside, achieving this objective usually leads to \gls{snr} degradation as suboptimal \gls{irs} configurations might be adopted for some \glspl{ue}. To mitigate this effect, we formulate a constrained optimization problem on the average cell sum capacity.
Specifically, we assume the following conditions:
\begin{enumerate}[label=\arabic*.]
    \item at most $Z$ \gls{irs} reconfigurations can occur per time frame;
    \item the \gls{gnb} serves $K$ \glspl{ue} by partitioning them into $Z$ disjoint subsets $\mathcal{U}_1,\ldots,\mathcal{U}_{Z}$, $Z\leq K$;
    \item for each \gls{ue} in $\mathcal{U}_z$, the same \gls{irs} configuration $\bm{\Phi}^{(z)}$ is used, i.e., $\bm{\Phi}_k = \bm{\Phi}^{(z)}, \forall \, k \in \mathcal{U}_z, \forall \, 1\leq z \leq Z$.
\end{enumerate}

Then, the achievable rate of \gls{ue} $k\in \mathcal{U}_z$ is
\begin{equation}
    R_k(\bm{\Phi}^{(z)}) = \log_2\left(1+\Gamma_k\big(\bm{\Phi}^{(z)}\big)\right),
    \label{eq:rate}
\end{equation}
where $\Gamma_k(\bm{\Phi}^{(z)})$ is the \gls{snr} experienced by the $k$-th \gls{ue} while configuration $\bm{\Phi}^{(z)}$ is adopted at the \gls{irs}, i.e., the configuration shared by all \glspl{ue} belonging to subset $\mathcal{U}_z$.

Let $\mathcal{I} = \{\bm{\Phi}^{(1)}, \bm{\Phi}^{(2)}, \ldots, \bm{\Phi}^{(Z)}\}$ be the set of \gls{irs} configurations corresponding to subsets $\mathcal{U}_1,\ldots,\mathcal{U}_{Z}$. The system sum capacity within a time frame is defined as
\begin{equation}\label{eq:sumrate}
    C(\mathcal{U}_1,\ldots,\mathcal{U}_{Z},\mathcal{I}) = B\sum_{z=1}^{Z} \sum_{k \in \mathcal{U}_z}R_k\big(\bm{\Phi}^{(z)}\big),
\end{equation} 
where $B$ is the transmission bandwidth. The optimization problem is then formulated as
\begin{subequations}\label{optproblem}
    \begin{IEEEeqnarray}{rCl}
        &\max_{\substack{\mathcal{U}_1,\ldots,\mathcal{U}_{Z}, \mathcal{I}}} \; &  C(\mathcal{U}_1,\ldots,\mathcal{U}_{Z},\mathcal{I}),\\
     & \text{s.t.}\; & \angle\big[\bm{\Phi}^{(z)}\big]_{n,n} \in \mathcal{P}_{\theta},\quad \forall\, n, z.\label{const_phaseshifters}
    \end{IEEEeqnarray}
\end{subequations}

Problem \eqref{optproblem} determines the optimal grouping strategy for the \glspl{ue} subsets $\mathcal{U}_1,\ldots,\mathcal{U}_{Z}$, and assigns the best \gls{irs} configuration accordingly. Therefore, \eqref{optproblem} is both continuous (i.e., the optimization of the IRS configuration) and combinatorial (i.e., the grouping of the \glspl{ue}), and can be thus classified as a \gls{minlp} problem.
Moreover, the following theorem holds.
\begin{theorem}
    The sum capacity maximization problem \eqref{optproblem} is NP-complete.
\end{theorem}
\begin{IEEEproof}
    First, we observe that the problem falls within the general NP class. This is because if \eqref{optproblem} is solved to find $\mathcal{U}_1, \ldots, \mathcal{U}_Z, \mathcal{I}$, both the sum capacity and the phase-shift constraints \eqref{const_phaseshifters} could be verified in polynomial time.
    To prove that the problem in NP-complete, we set $\mathcal{I}$, and consider the simplified problem
    \begin{equation}
        \max_{\substack{\mathcal{U}_1,\ldots,\mathcal{U}_{Z}}} \quad  C(\mathcal{U}_1,\ldots,\mathcal{U}_{Z},\mathcal{I}).
    \end{equation}
     This problem can be viewed as a multi-knapsack problem with different clusters $\mathcal{U}_1, \ldots, \mathcal{U}_Z$ as knapsacks, and the goal is to maximize the total system capacity.
     This is known to be NP-hard, as it is a generalization of the classic knapsack problem. 
     The original sum capacity maximization problem \eqref{optproblem}, where we consider the additional degrees of freedom of the \gls{irs} configurations, remains NP-hard, thus making the problem NP-complete.
\end{IEEEproof}

Given the inherent problem complexity, we adopt heuristic clustering algorithms to obtain approximated, though close-to-optimal, solutions, as described in Section~\ref{sec:optimization}.

\section{Heuristic Sum Capacity Maximization}
\label{sec:optimization}
In this section, we provide heuristic solutions to \eqref{optproblem}. First, we present two clustering-based approaches to identify and group \glspl{ue} with a similar optimal \gls{irs} configuration. Then, we solve the scheduling problem on the identified clusters with a \gls{tdma} approach \cite{anchora2012capacity}.
 We compute the \glspl{ue} clusters by first estimating the optimal {individual \gls{irs} configurations}, denoted as $\bm{\Phi}^*_k$, $1\leq k \leq K$, i.e., the \gls{irs} configurations leading to the maximum capacity for each \gls{ue} $k$, as described in Section~\ref{sec:ind_opt}.
 These configurations would solve \eqref{optproblem} for
$Z = K$, as in this case all \glspl{ue} are served in a \gls{tdma} fashion and with their optimal \gls{irs} configuration.
The phase coefficients of the optimal \gls{irs} configuration matrices are then chosen as the initial points of a procedure leveraging clustering algorithms in the $N_{\mathrm I}$-dimensional space, as explained in Section~\ref{sec:clust_tdma}.

\subsection{Optimal Individual IRS Configurations  }
\label{sec:ind_opt}
In \gls{mimo} systems, both the \gls{gnb} and the \glspl{ue} adopt properly tuned beamformers to match the signal transmissions and receptions to the spatial direction providing the highest channel gain~\cite{flamini2022towards}. For the optimization of the \gls{irs} configuration of each individual \gls{ue}, we adopt a procedure similar to that presented in \cite{Qian22joint}, focusing on single-stream transmissions and, without loss of generality, on \gls{ue} $k$.


For a given \gls{irs} configuration, the optimal beamforming vectors  $\bm{v}_k$ and $\bm{w}_k$ coincide with the singular vectors corresponding to the highest singular value of the wireless channel matrix. In particular, we calculate the \gls{svd} of the overall cascade channel matrix
\begin{equation}\label{eq:svd}
    \bm{G}_k \bm{\Phi}_k \bm{H} = \bm{U}\bm{\Sigma}\bm{V}^\dagger,
\end{equation}
where the right and left singular vectors of $\bm{G}_k \bm{\Phi}_k \bm{H}$ are the columns of $\bm{V}$ and $\bm{U}$, and the corresponding singular values are the diagonal entries of $\bm{\Sigma}$.

In our formulation, the \gls{irs} configuration $\bm{\Phi}_k$ is one of the optimization variables.  Indeed, given $\bm{v}_k$ and $\bm{w}_k$, we can solve
\begin{subequations}
	\begin{IEEEeqnarray}{rCl}
		\bm{\Phi}^*_k = &\argmax_{\substack{\bm{\Phi}_k}} \; & R_k(\bm{\Phi}_k), \label{optproblem_single}\\
		& \text{s.t.}\; & \angle[\bm{\Phi}_k]_{n,n} \in \mathcal{P}_{\theta},\quad \forall n,%n =1,\ldots,N_{\mathrm I} \, ,
	\end{IEEEeqnarray}
\end{subequations}
% \begin{subequations}\label{optproblem}
%     \begin{IEEEeqnarray}{rCl}
%         &\max_{\substack{\mathcal{U}_1,\ldots,\mathcal{U}_{Z}, \mathcal{I}}} \; &  C(\mathcal{U}_1,\ldots,\mathcal{U}_{Z},\mathcal{I}),\\
%      & \text{s.t.}\; & \angle\big[\bm{\Phi}^{(z)}\big]_{n,n} \in \mathcal{P}_{\theta},\quad \forall\, n, z.\label{const_phaseshifters}
%     \end{IEEEeqnarray}
% \end{subequations}
where $R_k$ is the achievable rate of user $k$, $1\leq k\leq K$, according to \eqref{eq:rate}.

The derivation of the optimal \gls{irs} configuration of user $k$ requires the alignment of the channel phase coefficients.
According to \cite{Swindlehurst2022Channel}, the cascade channel can be expressed as
\begin{equation}
\begin{split}    
    \bm{v}_k^{\mathrm T}\bm{G}_k \bm{\Phi}_k\bm{H}\bm{w}_k &= \vect\left(\bm{v}_k^{\mathrm T}\bm{G}_k \bm{\Phi}_k\bm{H}\bm{w}_k\right) \\ &= \left(\bm{w}_k^{\mathrm T}\bm{H}^{\mathrm T}\diamond\bm{v}_k^{\mathrm T}\bm{G}_k\right)\diag(\bm{\Phi}_k)
\end{split}
\end{equation}
where $\diamond$ denotes the Khatri-Rao product operator and $\diag(\bm{\Phi}_k)$ return the column vector with all the elements in the diagonal of $\bm{\Phi}_k$. 
Then, it is sufficient to observe that the \gls{snr} is maximized when the phase shifts introduced by the \gls{irs} are aligned with the phase shifts accumulated along the various paths, i.e., 
\begin{equation}
	\label{eq:theta_opt}
	\theta_{k, n} = -(\angle\left[\left(\bm{w}_k^{\mathrm T}\bm{H}^{\mathrm T}\diamond\bm{v}_k^{\mathrm T}\bm{G}_k\right)\right]_{n}), \quad \forall n.
\end{equation}
Note that, in general, we need to know the estimated phase shift of each component resulting from the Khatri-Rao product in \eqref{eq:theta_opt}, rather than the exact phase coefficients of $\bm{H}$ and $\bm{G}_k$. Moreover, as pointed out in \cite{Swindlehurst2022Channel}, for structured channel models adopted at \glspl{mmwave}, where multipath scattering is sparse and propagation is often dominated by strong specular components, the estimation of the separated channel matrices $\bm{H}$ and $\bm{G}_k$ can be simply accommodated by optimizing a limited number of parameters.

% In our formulation, the \gls{irs} configuration $\bm{\Phi}_k$ is one of the optimization variables.  Indeed, given $\bm{v}_k$ and $\bm{w}_k$, we can solve
% \begin{subequations}
%     \begin{equation}\label{optproblem_single}
%     \bm{\Phi}^*_k = \argmax_{\substack{\bm{\Phi}_k}} R_k(\bm{\Phi}_k), %\vspace{-10pt}
%     \end{equation}
%     \begin{alignat}{2}
%     & \text{s.t.}\; \quad & \angle[\bm{\Phi}_k]_{n,n} \in \mathcal{P}_{\theta},\quad n =1,\ldots,N_{\mathrm I} \, ,
%     \end{alignat}
%     \end{subequations}
% where $R_k$ is the achievable capacity of \gls{ue} $k$, $1\leq k\leq K$, according to \eqref{eq:rate}.
% By defining $\bm{s}_k = \bm{v}_k^{\mathrm T} \bm{G}$ and $\bm{u}_k = \bm{H} \bm{w}_k$, the signal power can be re-written as
% \medmuskip=-1mu
% \thickmuskip=-1mu
% \begin{equation}
%     \left|\bm{s}_k\bm{\Phi}_k \bm{u}_k\right|^2 = \left|\sum_{n = 1}^{N_{\mathrm I}} |[\bm{s}_k]_{n}||[\bm{u}_k]_{n}|e^{j(\angle[\bm{s}_k]_{n}+\theta_n+\angle[\bm{u}_k]_{n})}  \right|^2.
% \end{equation}
% \medmuskip=6mu
% \thickmuskip=6mu
% Then, it is sufficient to observe that the \gls{snr} is maximized when the phase shifts introduced by the \gls{irs} are aligned with the phase shifts accumulated along the various paths, i.e., 
% \begin{equation}
% \label{eq:theta_opt}
%     \theta_{k, n} = -(\angle[\bm{s}_k]_{n} + \angle[\bm{u}_k]_{n}), \quad \forall n.
% \end{equation}

Taking into account the possible quantization, the optimal phase shifts are given by
\begin{equation}
    \angle[\bm{\Phi}^*_k]_{n,n} \leftarrow
    \argmin_{\substack{\psi \in \mathcal{P}_{\theta}}}\left(\angle e^{j(\theta_{k, n}-\psi)}\right), \quad \forall n.  
\end{equation}

% \begin{figure}[t]
% \vspace*{-10pt}
\begin{algorithm}[t]%[H]
\caption{Iterative Alternate \gls{irs} Optimization}\label{alg:ind_opt}
\begin{algorithmic}[1]%\small
\Require $\bm{G}_k, \bm{H}$
\Ensure $\bm{\Phi}^*_k$
\State $t\gets 0$
\State $\bm{v}_k, \bm{w}_k \gets \bm{1}$
\Repeat
%\STATE$\bm{s}_k \gets \bm{v}_k^{\mathrm T} \bm{G}$, $\bm{u}_k \gets \bm{H}\bm{w}_k$
\State$	\theta_{k, n} \gets -(\angle\left[\left(\bm{w}_k^{\mathrm T}\bm{H}^{\mathrm T}\diamond\bm{v}_k^{\mathrm T}\bm{G}_k\right)\right]_{n}), \quad \forall n$
%\STATE$\theta_{k, n} \gets -(\angle[\bm{s}_k]_{n} + \angle[\bm{u}_k]_{n})$
\State$\angle[\bm{\Phi}_{k, t}]_{n,n} \gets \argmin_{\substack{\psi \in \mathcal{P}_{\theta}}}(\angle e^{j(\theta_{k, n}-\psi)})$
\State$\bm{U}, \bm{\Sigma}, \bm{V}^\dagger \gets$  SVD  of $\bm{v}_k^{\mathrm T}\bm{G}_k \bm{\Phi}_k \bm{H}\bm{w}_k $
\State$\bm{v}_k \gets$ column of $\bm{V}$ corresponding to \\\quad\quad the largest singular value
\State$\bm{w}_k \gets$ column of $\bm{U}$ corresponding to \\\quad\quad the largest singular value
\State$t \gets t+1$
\Until{$|R_k(\bm{\Phi}_{k, t}) - R_k(\bm{\Phi}_{k, t-1})| < \nu$}
\State $\bm{\Phi}^*_k \gets \bm{\Phi}_{k,t}$
\end{algorithmic}\normalsize
\end{algorithm}
% \vspace{-25pt}
% \end{figure}

To overcome the interdependence between optimal \gls{irs} configurations and beamforming vectors, we propose an iterative alternate optimization approach. We first estimate the optimal beamforming vectors for a given \gls{irs} configuration using \eqref{eq:svd}. Then, we plug the derived beamformers into \eqref{optproblem_single}, and obtain the corresponding optimal \gls{irs} configuration. We repeat this two-step procedure until convergence, which, for practical purposes, is assumed to be reached when the difference between the achievable rates $R_k$, $\forall k$, in two consecutive iterations is lower than a tolerance $\nu>0$. 
This procedure is summarized in Algorithm~\ref{alg:ind_opt}, where $t$ is the iteration index. The number of iterations grows with the numbers of antennas and \gls{irs} phase shifters. However, from preliminary simulations, and based on the set of parameters we considered (see Section~\ref{sec:numerical_results}), convergence is typically reached in less than $10$ iterations.


\subsection{Clustering-based TDMA Scheduling}
\label{sec:clust_tdma}
For an approximated but close-to-optimal solution to \eqref{optproblem}, we resort to a clustering-based approach.
Our proposed clustering algorithms estimate both the subsets of \glspl{ue} $\mathcal{U}_1,\ldots,\mathcal{U}_{Z}$, and the relative set of \gls{irs} configurations $\mathcal{I}$.
We operate on the \textit{phase vector space}, i.e., the points to be clustered are identified by the \gls{irs} phase shifts vector $\left[\angle{\phi_{1}},\ldots,\angle{\phi_{N_{\mathrm I}}}\right]^{\mathrm T} = \left[\theta_{1},\ldots,\theta_{N_{\mathrm I}}\right]^{\mathrm T}$,
which maps each \gls{irs} configuration $\bm{\Phi}$ to a point in $\left[-\pi,\pi\right)^{N_{\mathrm I}}$.
In case of quantized phase shifts, the phase vector space is a lattice in the continuous space $\left[-\pi,\pi\right)^{N_{\mathrm I}}$.

The general clustering-based procedure works as follows:
\begin{itemize}
    \item \emph{Step 1:} find $\bm{\Phi}^*_k,\, \forall k$, i.e., the optimal individual \gls{irs} configurations for each \gls{ue} as in Section \ref{sec:ind_opt};
    \item \emph{Step 2:} build \gls{ue} subsets $\mathcal{U}_z, \, z = 1, \ldots, Z$, by using a clustering algorithm, according to Sections~\ref{sec:dist_based} and \ref{sec:cap_based};
    \item \emph{Step 3:} assign $\bm{\Phi}^{(z)}$ to all \glspl{ue} $\in\mathcal{U}_z$.
\end{itemize}

The core idea of this procedure is to use clustering algorithms to group \glspl{ue}, and assign the respective \gls{irs} configurations, which are mapped to the \textit{centroid} of the cluster. 
In the case of quantized phase shifts, once the clustering procedure is performed, clusters may share the same centroid and be merged. Therefore, $Z$ represents the \textit{maximum number of clusters}, not the effective number.
Moreover, we remark that the procedure above does not rely on the assumption of perfect \gls{csi}, as the grouping strategy (Step 2) and the individual optimization step (Step 1) are performed independently. Nevertheless, in the case of imperfect \gls{csi}, the estimated individual optimal configurations may differ from the actual optimal configurations, leading to a suboptimal grouping.

In the following, we propose different techniques to build the clusters based on either a {distance metric} (Section~\ref{sec:dist_based}) or the {achievable rate}  (Section \ref{sec:cap_based}).

\section{Distance-Based Clustering Algorithms}
\label{sec:dist_based}
The class of distance-based clustering contains methods that group data points based on their similarity or dissimilarity according to a distance metric. 
This approach has several advantages, including the efficiency in handling large datasets, and the flexibility to adapt to many different scenarios of interest. 
However, distance-based clustering can be sensitive to the choice of the distance metric (which depends on the nature of the data and the clustering problem), and the initialization values. Moreover, in our specific case, it does not take into account the achievable rate, which is not directly related to the distance among the points in the phase vector space. 

Since the scalar field is the range $[-\pi,\pi)$, the adopted distance has to take into account the circularity of data.
However, the convergence to a local minimum for most of the clustering algorithms is guaranteed only if the points to be clustered belong to a Euclidean space. 
For distance-based algorithms, % which belong to this class, %, with the goal of measuring pairwise distances between \gls{irs} configurations $\bm{\theta}$, 
we thus define the bijective mapping function $f : \mathcal{P}_{\theta}^{N_{\mathrm I}} \to \mathbb{R}^{2N_{\mathrm I}}$ as
\begin{align}\label{map_theta}
    f(\bm{\theta}) &= f(\left[\theta_{1},\ldots,\theta_{N_{\mathrm I}}\right]) \nonumber \\ 
    %&= \left[ \cos(\theta_{1}),\ldots,\cos(\theta_{N_{\mathrm I}}), \sin(\theta_{1}),\ldots, \sin(\theta_{N_{\mathrm I}}) \right],
    &= \left[ \cos(\theta_{1}),\sin(\theta_{1}),\ldots,\cos(\theta_{N_{\mathrm I}}),\sin(\theta_{N_{\mathrm I}}) \right],
\end{align}
and then define the pairwise {distance} between two generic \gls{irs} configurations $\bm{\alpha}$ and $\bm{\beta}$ as
\begin{equation}\label{circdist}
    \delta(\bm{\alpha},\bm{\beta}) = \vert\vert f(\bm{\alpha}) - f(\bm{\beta}) \vert\vert,
\end{equation}
i.e., the Euclidean distance between the mapping on the unit $N_{\mathrm I}$-sphere of their respective phase vectors. In the following, with a slight abuse of notation, $f(\bm{\Phi})$ maps the phases of the complex entries in the diagonal of $\bm{\Phi}$ as in \eqref{map_theta}, and $\delta(\bm{\Phi}_1, \bm{\Phi}_2)$ denotes the pairwise distance between the phases of the elements in the diagonal of matrices $\bm{\Phi}_1$ and $\bm{\Phi}_2$.
The sum of squared distances is defined as
\begin{equation}\label{distancemin}
    J(\mathcal{U}_1,\ldots,\mathcal{U}_{Z},\mathcal{I})= \sum_{z=1}^Z\sum_{k \in \mathcal{U}_z}\delta\left(\bm{\Phi}_k^*, \bm{\Phi}^{(z)}\right)^2,
\end{equation}
and the distance-based clustering schemes are used to solve the following problem:
\begin{equation}\label{optproblemdist}
    \min_{\substack{\mathcal{U}_1,\ldots,\mathcal{U}_{Z}, \mathcal{I}}} \quad  J(\mathcal{U}_1,\ldots,\mathcal{U}_{Z},\mathcal{I}),
    \quad \quad \text{s.t.}\quad \eqref{const_phaseshifters}.
\end{equation}


We consider and compare some of the most popular distance-based clustering algorithms, namely, K-means, agglomerative hierarchical clustering, and K-medoids.  

%\vspace{2pt}\noindent
\emph{\gls{km}.}
\gls{km} clustering~\cite{rokach2005clustering} aims at finding $Z$ disjoint clusters minimizing the within-cluster squared Euclidean distances. Here, we consider the generalized Lloyd algorithm \cite{Kmeans}, which randomly selects $Z$ points in the space of phase vectors as the initial centroids. In our setup, to ensure optimal performance when $Z=K$, we force the algorithm initialization to a random selection among the phase vectors of the optimal individual \gls{irs} configurations derived in Section~\ref{sec:ind_opt}. Then, in the \textit{assignment step} \gls{km} assigns each data point to the closest centroid, according to the specified distance metric. 
In the subsequent \textit{update step}, the set of centroids is re-computed as the average of the data points that belong to each cluster. These steps are repeated until either convergence or a maximum number of iterations $I_{\mathrm max}^{\mathrm KM}$ is reached.
 
%\vspace{2pt}\noindent
\emph{Agglomerative \gls{hc}.}
The agglomerative \gls{hc}~\cite{murtagh2012algorithms} partitions a set of data points into disjoint clusters by iteratively merging points into clusters until a target number of partitions is met.
In our setup, clusters are initialized as the optimal phase vectors, which thus act as the respective centroids. Then, the average distance between all pairs of data points in any pair of clusters is evaluated. The closest pair of clusters are merged into a new single cluster, whose centroid is computed as the mean of its data points. The procedure is repeated until the number of clusters is $Z$. 

%\vspace{2pt}\noindent
\emph{\gls{kmed}.}
\gls{kmed}~\cite{kaufman1987clustering} is a clustering technique similar to \gls{km}, but instead of the mean of the data points within each cluster, it uses the medoid, i.e., the data point that is closest to the center of the cluster. In our setup, we consider the \gls{pam} method~\cite{van2003new}, which starts by randomly selecting $Z$ medoids among the optimal phase vectors and assigns each point to the cluster with the closest medoid. In each iteration, the algorithm evaluates potential \textit{swaps} of medoids with non-medoids. A swap is accepted only if it results in a lower value of the sum of the squared distances to all other data points within the same cluster. The algorithm continues until the medoids no longer change.

\begin{theorem}\label{th_distancebased}
    The proposed distance-based clustering techniques converge to a local minimum of \eqref{distancemin}.
\end{theorem}
\begin{IEEEproof}[Proof] 
\textcolor{blue}{The proof directly derives from the well-known results of clustering with Euclidean distance. The exact proofs for each of the considered algorithms under distance metric \eqref{circdist} are reported in the Appendix.}
\end{IEEEproof}

\section{Capacity-Based Clustering Algorithms}
\label{sec:cap_based}
The distance-based clustering techniques presented in Section~\ref{sec:dist_based} do not directly take into account the actual capacity achievable by the \glspl{ue}, which is a crucial factor for the sum capacity maximization~\eqref{optproblem}. Thus, in the following we propose original capacity-based clustering algorithms that go beyond the state of the art, namely \gls{cwc} (Section~\ref{sec:cwc}), \gls{oscbc} (Section~\ref{sec:clust_oscbc}), and \gls{icwc} (Section~\ref{sec:icwc}).

\subsection{Capacity-Weighted Clustering (CWC)}
\label{sec:cwc}

Similarly to distance-based clustering, also \gls{cwc} proceeds iteratively. However, the stopping condition is based on the variation of the sum capacity of each cluster, rather than on the distance between the centroids. 
In this approach, the clustering algorithm itself weighs the \glspl{ue} based on their achievable capacity, so that the parameters of the resulting clusters are closer to those preferred by the \glspl{ue} with higher rates, thus promoting the maximization of the sum capacity.

Let $\bm{\Phi}^{(z)}_{i}$ be the \gls{irs} configuration of cluster $\mathcal{U}_{z, i}$ at iteration $i$.
\glspl{ue} are initially sorted in decreasing order of achievable rate.  The algorithm then selects the $Z$ \glspl{ue} providing the highest achievable capacity based on the expression in \eqref{eq:rate} with their optimal \gls{irs} configurations.  Without loss of generality, we let $z=1, \dots, Z$ be the  index of those \glspl{ue}, and set $\bm{\Phi}_1^{(z)} = \bm{\Phi}^*_z$, $\forall\; 1\leq z\leq Z$, as the centroids of the initial clusters $\mathcal{U}_{1, 0}, \ldots, \mathcal{U}_{Z, 0}$. 
In the following, for simplicity, we denote with $z_{k, i}$ the cluster such that $k \in \mathcal{U}_{z, i}$.  
Each \gls{ue} $k > Z$ is assigned to the cluster whose centroid provides the lowest rate difference with respect to its ideal configuration. Let $R_k(\bm{\Phi}^*_k)$ be the maximum achievable rate of \gls{ue} $k$, obtained from the solution of problem \eqref{alg:ind_opt}. \gls{ue} $k$ is assigned to cluster
\begin{equation}
\label{cluster_assignment}
    z_{k, i} = \argmin_{\substack{z}} [R_k(\bm{\Phi}^*_k) - R_k(\bm{\Phi}^{(z)}_{i})] , 
\end{equation}
where $R_k(\bm{\Phi}^{(z)}_i)$ is the rate achieved by \gls{ue} $k$ adopting the \gls{irs} configuration of cluster $z$ at iteration $i$.
Note that, despite being always non-negative, the rate difference in \eqref{cluster_assignment} cannot be considered a distance metric as, in general, it does not satisfy the triangle inequality.
However, we prove that, as the distance from the optimal configuration increases, the corresponding rate decreases, thus supporting the use of the rate difference as a clustering criterion.

\begin{theorem}
Given the optimal IRS configuration $\bm{\Phi}^*_k$, the rate $R_k(\bm{\Phi})$ is monotonically decreasing with respect to the magnitude of any phase shifts error $\epsilon$.
\end{theorem}
\begin{IEEEproof}
    Let $\epsilon\in [-\pi, \pi]$ be an arbitrary error phase shift, and consider the configuration $\bm{\Phi}_k^\epsilon = \bm{\Phi}^*_k \bm{E}$, where $\bm{E} = \diag(e^{j\epsilon}, 1,\ldots,1)$, i.e., the suboptimal configuration where only the first \gls{irs} element is affected by the error $\epsilon$.
Assuming, without loss of generality, that $N_{\mathrm g}=N_k=1$ and $\sigma_x=\sigma_n=1$, the rate $R_k(\bm{\Phi}_k^\epsilon)$ is proportional to $\Gamma_k(\bm{\Phi}_k^\epsilon)$ when using configuration $\bm{\Phi}_k^\epsilon$. The \gls{snr} $\Gamma_k(\bm{\Phi}_k^\epsilon)$ can be written~as
\begin{align}
\label{eq:rate_error}
\Gamma_k(\bm{\Phi}_k^\epsilon) &= |\bm{g}_k \bm{\Phi}^*_k\bm{E} \bm{h}|^2 \\
&= \left|[\bm{g}_k]_1 [\bm{\Phi}^*_k\bm{E}]_{1,1} [\bm{h}]_1 + A \right|^2,
\end{align}
where $A = \sum_{n=2}^{N_{\mathrm I}}[\bm{g}_k]_n [\bm{\Phi}^*_k]_{n,n} [\bm{h}]_n$. Since $\bm{\Phi}^*_k$ is the optimal configuration, it satisfies \eqref{eq:theta_opt}. It follows that $A \in \mathbb{R}^+$, so \eqref{eq:rate_error} can be further manipulated into
\begin{align}
& \Gamma_k(\bm{\Phi}_k^\epsilon) = | |[\bm{g}_k]_1||[\bm{h}]_1|e^{j \epsilon} + A |^2 \\  
&= A^2 + (|[\bm{g}_k]_1||[\bm{h}]_1|)^2 + 2A|[\bm{g}_k]_1||[\bm{h}]_1|\cos(\epsilon).
\end{align}
Finally, we evaluate the sign of the derivative of $\Gamma_k(\bm{\Phi}_k^\epsilon)$ with respect to the error $\epsilon$ as
\begin{equation}
   \frac{\partial\Gamma_k(\bm{\Phi}_k^\epsilon)}{\partial\epsilon} = -2A|[\bm{g}_k]_1||[\bm{h}]_1|\sin(\epsilon),
\end{equation}
and observe that $\Gamma_k(\bm{\Phi}_k^\epsilon)$, and therefore $R_k(\bm{\Phi}_k^\epsilon)$, is strictly decreasing for $0< |\epsilon|\leq \pi$.
\end{IEEEproof}

After all the remaining \glspl{ue} have been assigned to the corresponding clusters, the coordinates of the centroids are updated.
At iteration $i+1$, the new \gls{irs} configuration (centroid) of cluster $\mathcal{U}_{z, i+1}$ is computed as the average of the data points in the cluster, weighted by their achievable rate, i.e.,
\begin{equation}\label{centroidCWC}
    %\bm{\Phi}^{(z)}_{i+1} = \frac{\sum_{k \in \mathcal{U}_z}  \bm{\Phi}^*_kR_k(\bm{\Phi}^{(z)}_{i})}{\sum_{k \in \mathcal{U}_z}R_k(\bm{\Phi}^{(z)}_{i})}.
    \bm{\Phi}^{(z)}_{i+1} = f^{-1}\left(\frac{\sum_{k \in \mathcal{U}_z}  f(\bm{\Phi}^*_k)R_k(\bm{\Phi}^*_k)}{\sum_{k \in \mathcal{U}_z}R_k(\bm{\Phi}^*_k)}\right).
\end{equation}
Also, in the case of phase shift quantization, an additional approximation step must be performed as
\begin{equation}\label{centroid_discr}
    \angle[\bm{\Phi}^{(z)}_{i+1}]_{n,n} \leftarrow
    \argmin_{\substack{\psi \in \mathcal{P}_{\theta}}}\left(\angle e^{j(\angle[\bm{\Phi}^{(z)}_{i+1}]_{n,n} - \psi)}\right), \quad \forall n.
\end{equation}
This two-step procedure is repeated until convergence, which is reached when the rate difference between two consecutive iterations is lower than the sum capacity tolerance $\mu>0$.

%\begin{figure}[t]
%\vspace{-10pt}
\begin{algorithm}[t]%[H]
\caption{\gls{cwc} Algorithm}\label{alg:clustering_cwc}
\begin{algorithmic}[1]%\small
\Require{$Z$, $\bm{H}$, $\bm{G}_k,\;  \forall k$}
\Ensure{$\mathcal{U}_1, \ldots, \mathcal{U}_Z$, $\mathcal{I}$}
\State Compute $\bm{\Phi}^*_k, \forall k$ with the procedure of Algorithm~\ref{alg:ind_opt}
%\STATE $i \gets 1$
\State Sort the \glspl{ue} in decreasing order of  $R_k(\bm{\Phi}^*_k)$
\State Select the $Z$ \glspl{ue} providing the highest $R_k(\bm{\Phi}^*_k)$,  
\State Set $\bm{\Phi}_1^{(z)} = \bm{\Phi}^*_k$, $z = 1,\ldots, Z$ as the initial centroids.
\Repeat
\For{each \gls{ue} $k$}
\State $z_{k, i} \gets \argmin_{\substack{z}} R_k(\bm{\Phi}^*_k) - R_k(\bm{\Phi}^{(z)}_{i})$
\EndFor
\For{each cluster $z$}
\State Compute $\bm{\Phi}^{(z)}_{i+1}$ as per \eqref{centroidCWC}, \eqref{centroid_discr}
\EndFor
\State$i \gets i+1$
\Until{$\left|\sum_{k \in \mathcal{U}_z} R_k(\bm{\Phi}^{(z)}_{i})-\sum_{k \in \mathcal{U}_z} R_k(\bm{\Phi}^{(z)}_{i-1})\right|<\mu$}
\State Assign $\bm{\Phi}^{(z)}$ to all $k\in\mathcal{U}_z$.
\end{algorithmic}\normalsize
\end{algorithm}
%\vspace{-25pt}
%\end{figure}

The rationale behind the algorithm is that, based on the initial centroid assignment, the \glspl{ue} experiencing the best channel conditions, i.e., those dominating the system sum capacity, are initially served with their optimal (individual) \gls{irs} configurations. 
Even after the adjustment of the clusters, these \glspl{ue} will always get the largest weight coefficient within the cluster. The remaining \glspl{ue}, instead, will be penalized by the configuration constraints, but their impact on the sum capacity will be limited.
The whole workflow of the \gls{cwc} procedure is summarized in Algorithm~\ref{alg:clustering_cwc}.

\subsection{One-Shot Capacity-Based Clustering (OSCBC)}
\label{sec:clust_oscbc}
The main drawback of \gls{cwc} presented in Section~\ref{sec:cwc} is that it requires solving problem~\eqref{cluster_assignment} at each iteration, relative to all the \glspl{ue} in each cluster. 
Considering massive \gls{mimo} systems, the \gls{cwc} procedure could become exceedingly complex, as it requires the \gls{svd} computation of extremely large matrices.
Therefore, we propose another lower-complexity clustering algorithm, denoted as \gls{oscbc}.

As in \gls{cwc}, also in \gls{oscbc}:
\begin{enumerate*}[label=(\textit{\roman*})]
\item the \glspl{ue} are sorted in decreasing order of achievable rate; \item the $Z$ \gls{irs} configurations of the $Z$ \glspl{ue} experiencing the highest rates are chosen as initial centroids for the clusters; and \item  the remaining \glspl{ue} are assigned to the closest centroid in terms of circular distance, as per \eqref{circdist}. 
\end{enumerate*}
Then, compared to \gls{cwc}, instead of recomputing the coordinates of the centroids at each iteration until convergence, the algorithm stops right after the initial association.
Therefore, with \gls{oscbc} the computed centroids are the optimal configurations relative to the $Z$ \glspl{ue} achieving the highest individual rate, which provides suboptimal (non-optimized) performance for the rest of the \glspl{ue} in the clusters.

\subsection{Inverse Capacity-Weighted Clustering (ICWC)}
\label{sec:icwc}

The \gls{cwc} algorithm is designed to optimize the capacity of the \glspl{ue} experiencing the best channel conditions and is unfair to the other \glspl{ue} in the system, which may use suboptimal \gls{irs} configurations. 
Therefore, we propose an additional variation of \gls{cwc}, named \gls{icwc}, with the goal of achieving higher fairness among the \glspl{ue} in the system.
In \gls{icwc}, while the cluster association principle of \eqref{cluster_assignment} is preserved, the initial condition is reversed. Specifically:
\begin{enumerate*}[label=(\textit{\roman*})]
\item \glspl{ue} are sorted in increasing order of achievable rate;
\item the initial configurations of the clusters $\bm{\Phi}_1^{(z)} = \bm{\Phi}^*_z$, $z = 1,\ldots, Z$, are based on the optimal configurations of the \glspl{ue} with the worst channel conditions. 
\end{enumerate*}
The remaining $k>Z$ \glspl{ue} are associated as in \eqref{cluster_assignment}.
Then, at iteration $i$, the \gls{irs} configuration is updated as
\begin{equation}\label{centroidICWC}
    \bm{\Phi}^{(z)}_{i+1} = f^{-1}\left(\frac{\sum_{k \in \mathcal{U}_z}  \bm{\Phi}^*_kR_k^{-1}(\bm{\Phi}^*_k)}{\sum_{k \in \mathcal{U}_z}R_k^{-1}(\bm{\Phi}^*_k)}\right),
\end{equation}
and the discretization step \eqref{centroid_discr} is performed (if needed). 
As in \gls{cwc}, convergence is achieved if the rate difference between two consecutive iterations is lower than the tolerance $\mu$.  
While \gls{icwc} obtains lower sum capacity than \gls{cwc}, it can provide significant improvements in terms of fairness, especially from the perspective of the \glspl{ue} with the worst channel conditions.

\subsection{Computational Complexity}\label{complexity}

\begin{table} 
\centering
\caption{Computational complexity of distance-based vs. capacity-based clustering.}
\label{tab:clustering}
  \small
\begin{tabular}{lc}
  \toprule
Clustering algorithm & Computational complexity\\\midrule
\gls{km} (Lloyd) & $O(IZKN_{\mathrm I})$\\
\gls{kmed} (\gls{pam})& $O(Z^3K^2N_{\mathrm I})$\\
\gls{hc} & $O(K^3N_{\mathrm I})$ \\ \midrule
\gls{cwc}/\gls{icwc} & $O(IZKN_{\mathrm g}N_{\mathrm I}^2)$\\
\gls{oscbc} & $O(Z(K-Z)N_{\mathrm I})$\\
\bottomrule
\end{tabular}
%\vspace{-15pt}
\end{table}

The computational complexity is evaluated as the number of iterations required for the clustering algorithms to: \begin{enumerate*}[label=(\textit{\roman*})] 
\item obtain the optimal \gls{irs} configuration of each \gls{ue};
\item partition the \glspl{ue} into disjoint subsets, or clusters, based on distance or capacity metrics; and 
\item for each cluster, find the best \gls{irs} configuration to serve the corresponding \glspl{ue}.
\end{enumerate*}

Specifically, at each iteration, the main source of complexity is the computation of the overall cascade channel matrix $\bm{G}_k \bm{\Phi}_k \bm{H}$, which has complexity $O\big(N_{\mathrm g}N_{\mathrm I}^2 + N_{\mathrm g}N_{\mathrm I}N_{\mathrm U}\big)$.
Additionally, in the case of quantized \gls{irs} phase shifts, after obtaining the optimal beamformers, the optimal phase shifts for the \gls{irs} are obtained through an exhaustive search over the set of possible phase shifts $\mathcal{P}_{\theta}$, yielding a complexity $O(2^b N_{\mathrm I})$.

Notice that different clustering algorithms, in general, require a different number of iterations $I$ to reach convergence, thus possibly introducing practical limitations. Moreover, the complexity introduced in each iteration depends on the clustering algorithm itself. 
In Table~\ref{tab:clustering} and in the following text we characterize the computational complexity of each of the clustering algorithms presented in Sections~\ref{sec:dist_based} and \ref{sec:cap_based}.

%\vspace{2pt}\noindent
\emph{Distance-based clustering.}
These algorithms do not require specific initialization.  
For \gls{km}, based on the Lloyd implementation in \cite{Kmeans}, each iteration involves calculating the distances between data points and centroids. As a result, the computational complexity is influenced by the number of iterations required for convergence, the number of data points, the number of clusters, and the dimensionality of data, resulting in an overall complexity $O(IZKN_{\mathrm I})$.  
\gls{kmed} can be solved with the \gls{pam} algorithm~\cite{van2003new}, so the computational complexity is $O(Z^3K^2N_{\mathrm I})$ due to the pairwise distance computations between data points and medoids. Finally, the computational complexity of the agglomerate \gls{hc} is primarily determined by the computation of pairwise distances among all data points, resulting in a total complexity $O(K^3N_{\mathrm I})$ \cite{xu2015comprehensive}.

%\vspace{2pt}\noindent
\emph{Capacity-based clustering.}
The complexity of the \gls{oscbc} algorithm is dominated by the centroid assignment upon initialization, which has complexity $O\big(Z(K-Z)N_{\mathrm I}\big)$. Instead, for the CWC and ICWC algorithms, the complexity is $O(IZKN_{\mathrm g}N_{\mathrm I}^2)$, as demonstrated in the following theorem.

\begin{theorem}
The time complexity of \gls{cwc} and \gls{icwc} scales quadratically with $N_{\mathrm I}$ as $O(IZKN_{\mathrm g}N_{\mathrm I}^2$).
\end{theorem}
\begin{IEEEproof}
Capacity-based clustering requires an initialization stage where the algorithm selects the $Z$ \glspl{ue} providing the highest (or lowest) $R_k\big(\bm{\Phi}^*_k\big)$, resulting in a complexity of $O(K\log K)$ due to the sorting of $K$ scalars.
In the subsequent iterations:
\begin{enumerate}
    \item Both \gls{cwc} and \gls{icwc} compute the rate difference between each \gls{ue} and the $Z$ centroids. The complexity of computing $R_k(\bm{\Phi}^{(z)}_{i})$ can be dominated either by the matrix multiplication in \eqref{snr_k}, or by the \gls{svd} for the single stream beamforming which require, respectively, $O\big(N_{\mathrm g}N_{\mathrm I}^2 + N_{\mathrm g}N_{\mathrm I}N_{\mathrm U}\big)$ and $O\big(N_{\mathrm g}N_{\mathrm U}\min(N_{\mathrm g},N_{\mathrm U})\big)$ operations for each \gls{ue} and each centroid.
    \item The computation of the centroids as per \eqref{centroidCWC}-\eqref{centroidICWC} requires $N_{\mathrm I}+1$ scalar operations per \gls{ue}, which has negligible complexity with respect to the rate computation.
 \end{enumerate}
In typical \gls{irs}-assisted systems, $N_{\mathrm I}\gg N_{\mathrm g}>N_{\mathrm U}$. Therefore, the complexity at each iteration is dominated by the channel matrix product, and the overall algorithm complexity is $O(IZKN_{\mathrm g}N_{\mathrm I}^2)$. 
\end{IEEEproof}

\section{Numerical Results}\label{sec:numerical_results}

After presenting our various simulation scenarios and evaluation metrics in~Sections~\ref{sub:sim-params} and \ref{sec:metrics}, respectively, we assess in Section~\ref{sub:results} the scheduling performance of an \gls{irs}-assisted network with practical constraints.

\subsection{Simulation Parameters}
\label{sub:sim-params}

\begin{table}[t!]
  \caption{Simulation parameters.}
  \label{Tab:parameters-irs-block}
  \small
  \centering
  \begin{tabular}{lc}
    \toprule
    Parameter                  & Value \\ \midrule
    Carrier frequency		   & $28$~GHz	\\
    Total bandwidth	($B$)		   & $100$~MHz \\
    Noise power spectral density  & $-174$~dBm/Hz \\
    Number of \glspl{ue} ($K$) & $100$ \\
    \gls{gnb} antenna array ($N_{\mathrm g}$)    & $8$H$\times8$V \\
    \gls{gnb} transmit  power	   & $33$~dBm \\
     \gls{ue} antenna array ($N_{\mathrm U}$)    & $2$H$\times1$V \\
    \multirow{2}{*}{\gls{irs} elements ($N_{\mathrm I}$)}&\{$10$H$\times20$V, $20$H$\times40$V,\\&\quad $40$H$\times80$V, $60$H$\times120$V\}\\
    Phase shift quant. bits ($b$)            & \{unquantized, $1$-bit, $2$-bits\}\\%, $5$
    \Acrshort{los} probability ($p_{\mathrm LoS}$)            & Eq.~\eqref{losprob}\\
    Individual rate opt. tolerance ($\nu$)            & $10^{-6}$~[bit/s/Hz]\\
    \gls{km} max. iterations ($I_{\mathrm max}^{\mathrm KM}$)            & $50$\\
    \gls{cwc}/\gls{icwc} rate tolerance ($\mu$)        & $10^{-3}$~[bit/s]\\
    \bottomrule
  \end{tabular}
  %\vspace{-15pt}
\end{table}

Our simulation parameters are reported in Table~\ref{Tab:parameters-irs-block}.

%\vspace{2pt}\noindent
\emph{Scenario.} All devices are assumed to lie on a 2D plane, and  we consider an \gls{umi} scenario, according to the 3GPP nomenclature~\cite{3gpp.38.901}, with the \gls{gnb} placed at the center. According to the 3GPP specifications, the coverage area of the \gls{gnb} is characterized by an average radius of $167$~m and is assumed to lie in the positive $x$-axis region.

We assume that $K=100$ \glspl{ue} are randomly deployed according to a uniform distribution within the cell area, to be served in downlink by the \gls{gnb}, assisted by an \gls{irs} at coordinates  $(75, 100)$~m. 
The \gls{gnb} is equipped with a \gls{upa} with $8$H$\times8$V antennas (i.e., $N_{\mathrm g}=64$), and the \glspl{ue} with \glspl{ula} of $2$H$\times1$V antennas (i.e., $N_{\mathrm U} = 2$). 
For the \gls{irs}, if not otherwise specified, we adopt a $40$H$\times80$V reflective panel ($N_{\mathrm I}= 3200$).

%\vspace{2pt}\noindent
\emph{Channel and Frame Structure.}
The system operates at a carrier frequency of $28$~GHz (that is in the lower part of the \gls{mmwave} bands), the transmission power at the \gls{gnb} is set to $33$~dBm, the noise power spectral density at the receivers is $-174$~dBm/Hz,  and the total system bandwidth is $100$~MHz. We consider the fourth numerology of the \gls{nr} frame structure \cite{3gpp.38.211}, wherein each $10$~ms frame is split into $160$ slots. With this assumption, as already pointed out in Section~\ref{sec:system_model}, channels can be considered constant over the entire frame duration.
We consider the \gls{3gpp} TR~38.901 spatial channel model~\cite{3gpp.38.901}, which supports a wide range of frequencies, from $0.5$ to $100$ GHz (and including therefore our carrier frequency of $28$~GHz), and can be integrated with realistic beamforming models.
As such, channel matrices, and multipath fading, are computed based on the superposition of $N$ different clusters, each of which consists of $M$ rays that arrive (depart) to (from) the antenna arrays with specific angles and powers.
Based on~\cite{3gpp.38.901}, and using the simplifications proposed in~\cite{zugno20implementation}, the generic entry $[\bm{A}]_{pq}$ of the channel matrix can then be computed as:
\begin{equation}
\label{eq:ch_model_full}
\begin{aligned}
[\bm{A}]_{pq} = \; &\gamma \sum_{n=1}^{N} \sqrt{\frac{P_{n}}{M}} \sum_{m=1}^{M} \overline{\mathbf{F}}_{r x}\left(\theta_{n, m}^{A}, \phi_{n, m}^{A}\right) \\
& \times\left[\begin{array}{cr}
e^{j \Phi_{n, m}^{\theta, \theta}} & \sqrt{K_{n, m}^{-1}} e^{j \Phi_{n, m}^{\theta, \phi}} \\
\sqrt{K_{n, m}^{-1}} e^{j \Phi_{n, m}^{\phi, \theta}} & e^{j \Phi_{n, m}^{\phi, \phi}}
\end{array}\right] \\
& \times \overline{\mathbf{F}}_{tx}\left(\theta_{n, m}^{D}, \phi_{n, m}^{D}\right) \\
& \times e^{j \overline{\mathbf{k}}_{rx, n, m}^{\mathrm T} \overline{\mathbf{d}}_{rx, p} e^{j \overline{\mathbf{k}}_{tx, n, m}^{\mathrm T} \overline{\mathbf{d}}_{tx, q}}},
\end{aligned}
\end{equation}
where $\gamma$ is the \gls{lsfc} of the considered link, which incorporates the path loss and shadowing terms. For a complete description of the remaining terms appearing in \eqref{eq:ch_model_full} we refer the interested reader to~\cite{zugno20implementation}.
Specifically, while the \gls{gnb} and the \gls{irs} can be assumed to operate in \gls{los}, the path loss between a generic \gls{ue} $k$ and the \gls{irs} is modeled based on the following channel conditions:

\noindent $\bullet$ \emph{\gls{nlos}}: \gls{ue} $k$ is in \gls{nlos} with the~\gls{irs};

\noindent $\bullet$ \emph{\gls{dlos}}: \gls{ue} $k$ is in \gls{los} with the \gls{irs};

\noindent $\bullet$ \emph{\gls{plos}}: the \gls{irs}-\gls{ue} $k$ link is in \gls{los}$\vee$\gls{nlos} with respective probabilities $p_k^{\mathrm LoS}(d_k)$ $\vee$ $1{-}p_k^{\mathrm LoS}(d_k)$, with
\begin{equation}
\label{losprob}
p_k^{\mathrm LoS}(d_k) = 
\begin{cases}
1 \quad &\hbox{if} \quad d_k{\leq}18, \\
\frac{18}{d_k} {+} \left(1{-}\frac{18}{d_k}\right)e^{-\frac{d_k}{36}} \quad &\hbox{if} \quad d_k{>}18,
\end{cases}
\end{equation}
where $d_k$ is the distance (in m) between the \gls{irs} and \gls{ue} $k$. In the considered \gls{umi} scenario, and based on 3GPP specifications~\cite{3gpp.38.901}, the average \gls{los} probability in~\eqref{losprob} is~$0.35$.

For each wireless link, based on the presence of the \gls{los} component, the path loss is then derived according to \cite[Table 7.4.1-1]{3gpp.38.901}, with shadowing standard deviation set to $\sigma_{\mathrm SF}=0$.  
For the optimal individual \gls{irs} configuration (Section~\ref{sec:ind_opt}), we set $\nu = 10^{-6}$~[bit/s/Hz].

%\vspace{2pt}\noindent
\emph{Clustering algorithms.}
In the following subsections, we present extensive simulation results to compare the performance of distance-based (\gls{km}, \gls{hc}, \gls{kmed}) vs. capacity-based (\gls{cwc}, \gls{oscbc}, \gls{icwc}) clustering algorithms to perform scheduling in an \gls{irs} system with reconfiguration constraints.
The \gls{km} clustering has been implemented with the Lloyd algorithm \cite{Kmeans} with a maximum number of $I_{\mathrm max}^{\mathrm KM}=50$ iterations. 
Instead, for both \gls{cwc} and \gls{icwc}, we set $\mu = 10^{-3}$~[bit/s].

As an upper bound to the system performance, we also consider an ``unclustered'' scheduling, wherein we assume that all \glspl{ue} are served with their optimal \gls{irs} configuration. This scheduling clearly violates the constraint on the maximum numbers of reconfiguration per frame,  but can be regarded as the limit case when $Z = K$, i.e., all \glspl{ue} belong to a cluster with cardinality one. As such, it is a suitable approach for benchmarking the performance of more practical schemes.

\subsection{Performance Metrics}
\label{sec:metrics}

The performance of the proposed clustering-based scheduling techniques is evaluated in terms of average sum capacity and fairness, as a function of the numbers of both clusters and \glspl{ue}, under different channel conditions, \gls{irs} dimensions, and degrees of quantization for the phase shifts.

%\vspace{2pt}\noindent
\emph{Average sum capacity.} 
It is derived from \eqref{eq:sumrate} as
\begin{equation}
    \bar{C} = \frac{1}{K}\mathbb{E}\left[C(\mathcal{U}_1,\ldots,\mathcal{U}_{Z},\mathcal{I})\right],
\end{equation}
where the expectation is computed across the different channel realizations. Moreover, as each \gls{ue} is served in its specific slot, we average over the  \gls{tdma} frame length, dividing the empirical expectation by the number of \glspl{ue} (slots)~$K$.  

%\vspace{2pt}\noindent
\emph{Fairness.}
We consider the $95$\% percentile of the achieved individual user capacity, computed as
\begin{equation}\label{C95}
    C_{95\%} = \frac{B}{K}\inf\{x: {\mathrm CDF}(x) \geq 0.95\},
\end{equation}
where ${\mathrm CDF}(\cdot)$ is the empirical cumulative distribution function  of $R_k(\bm{\Phi}^{(z)})$, $\forall k, z$.
Notice that the $95$\% percentile of the user capacity is a practical and meaningful way to evaluate fairness, as it measures the performance of the majority of the \glspl{ue}, excluding only the top 5\%.

\subsection{Scheduling Performance}
\label{sub:results}

In this section, we compare the \gls{irs} scheduling performance considering distance-based vs. capacity-based clustering, and as a function of different channel conditions, reconfiguration constraints, and degrees of quantization of the phase shifts. 

\begin{figure}[t]
    \centering
       \setlength\fwidth{0.82\columnwidth}
    \setlength\fheight{0.55\columnwidth}
     \input{Figures/IrsClustering/sumcap0bitslos.tex}
     %\vspace{-20pt}
    \caption{Average sum capacity as a function of the maximum number of clusters $Z$, for an unquantized $40$H$\times80$V \gls{irs}, and considering a \gls{plos} channel for the \gls{irs}-\glspl{ue} links.}
    \label{fig:sumcap0bitlos}
    %\vspace{-10pt}
\end{figure}

%\vspace{2pt}\noindent
\emph{Impact of the clustering algorithm.}
First, Fig.~\ref{fig:sumcap0bitlos} displays the average sum capacity $\bar{C}$ per slot as a function of the number of clusters $Z$, for unquantized \gls{irs} phase shifts, and considering a \gls{plos} channel for the \gls{irs}-\glspl{ue} links. 
It is evident that all the scheduling policies perform better whenever $Z$ increases, and converge to the ``unclustered'' policy when $Z=K$. In fact, increasing the number of clusters corresponds to a smaller intra-cluster average distance, which eventually becomes zero when $Z=K$. 
Among the considered clustering policies, \gls{cwc} and \gls{oscbc} provide the highest sum capacity, as they are designed to maximize $\bar{C}$, and choose the \gls{irs} configurations of the \glspl{ue} that achieve the highest rate. Instead, distance-based clustering achieves worse performance as it does not exploit the knowledge of the rate achievable with different \gls{irs} configurations when building the clusters.
As expected, \gls{icwc} is designed to promote fairness, thus performs worse than both \gls{cwc} and \gls{oscbc} in terms of sum capacity; still, it achieves similar performance as distance-based clustering.
Finally, the gap between \gls{cwc} and \gls{oscbc} is almost negligible: this implies that a single iteration in the clustering process is enough to achieve good sum capacity, while also promoting lower computational complexity as reported in Table~\ref{tab:clustering}, which demonstrates the good scalability of the proposed techniques.


\begin{figure}[t]
    \centering
    \setlength\fwidth{0.82\columnwidth}
    \setlength\fheight{0.55\columnwidth}
    \input{Figures/IrsClustering/quantile0bitslos.tex}
    %\vspace{-20pt}
    \caption{$95$\% percentile of the user capacity as a function of the maximum number of clusters   $Z$, for an unquantized $40$H$\times80$V \gls{irs}, and considering a \gls{plos} channel for the \gls{irs}-\glspl{ue} links.}
    %\vspace{-10pt}
    \label{fig:quantile0bitsplos}
\end{figure}


Fig.~\ref{fig:quantile0bitsplos} compares the fairness performance of the different clustering algorithms, measured as the $95$\% percentile of the average sum capacity $C_{95\%}$, as a function of the maximum number of clusters $Z$ in \gls{plos} conditions. 
Our results identify \gls{icwc} as the best clustering approach in terms of fairness, which comes at the cost of a lower sum capacity,  as shown in Fig.~\ref{fig:sumcap0bitlos}. 
Therefore, there exists a trade-off between the achievable sum capacity and fairness. 
We also observe that \gls{oscbc} achieves very low fairness, as the \glspl{ue} with worst channel conditions are forced to aggregate to the strongest \glspl{ue}, thus via a suboptimal \gls{irs} configuration. On the other hand, we see that \gls{cwc} is more than acceptable in terms of fairness, and achieves comparable performance than most of the distance-based clustering algorithms.
Furthermore, $C_{95\%}$ increases as $Z$ increases, and eventually approaches the ``unclustered'' baseline for $Z=K$. This is due to the fact that the \gls{los} probability in the \gls{plos} scenario increases with the number of clusters, i.e., as the inter-cluster distance becomes smaller, which permits to experience better channel conditions, thus a higher capacity, even for the worst \glspl{ue}.
Finally, despite performing worse than their capacity-based counterparts, the distance-based methods are a viable alternative for constrained \gls{irs} control nodes thanks to their lower computational complexity. In such cases, \gls{hc} is to be preferred for sum-capacity maximization, while \gls{km} is the best alternative to capacity-based algorithms when the $95$\% percentile of the average sum capacity represents the metric of interest.

\begin{figure}[t]
    \centering
    \setlength\fwidth{0.82\columnwidth}
    \setlength\fheight{0.55\columnwidth}
    \input{Figures/IrsClustering/Los_comparison_sumcap}
    %\vspace{-20pt}
    \caption{Average sum capacity as a function of the maximum number of clusters $Z$, for $N_{\mathrm I}=3200$, unquantized phase shifts, and for different channel conditions.}
    \label{fig:sumcaplos}
    %\vspace{-10pt}
\end{figure}

\begin{figure}[t]
    \centering
       \setlength\fwidth{0.82\columnwidth}
    \setlength\fheight{0.55\columnwidth}
    \input{Figures/IrsClustering/Los_comparison_quant}
    %\vspace{-10pt}
    \caption{$95$\% percentile of the user capacity as a function of the maximum number of clusters  $Z$,  for $N_{\mathrm I}=3200$, unquantized phase shifts, and for different channel conditions.} %The zoom inside the figure is for $50\leq Z \leq 90$.}
    %\vspace{-10pt}
    \label{fig:quantitelos}
\end{figure}

%\vspace{2pt}\noindent
\emph{Impact of the channel.}
From the above results, we concluded that distance-based clustering provides lower sum capacity and fairness compared to capacity-based scheduling, so the rest of our simulation campaign has been focused on the latter. 
Figs.~\ref{fig:sumcaplos} and \ref{fig:quantitelos} display the average sum capacity and the $95$\% percentile, respectively, for \gls{cwc}, \gls{icwc}, and \gls{oscbc} in different channel conditions.
First, we observe that in the \gls{dlos} scenario, where \glspl{ue} are in \gls{los} with the \gls{irs}, the sum capacity is up to 2.6 (2.4) times higher than in the \gls{nlos} (\gls{plos}) scenario for $Z=K$.
This is mainly due to the fact that \gls{nlos} links experience
\begin{enumerate*}[label=(\textit{\roman*})]
  \item a higher path loss, and
  \item the lack of a dominant multipath component, thus of a clear steering direction for the \gls{irs} beam, which deteriorates the link quality. \end{enumerate*} 
In particular, in the \gls{plos} scenario the \gls{los} probability decreases exponentially with the distance, therefore, the \glspl{ue} that are far from the \gls{irs} typically operate in \gls{nlos}.
For similar reasons, both \gls{cwc} and \gls{icwc} in the \gls{dlos} scenario start to reach stability in terms of capacity with a relatively lower number of clusters than in the \gls{plos} and \gls{nlos} scenarios. 

As expected, \gls{oscbc} performs worse than its competitors, and the gap is even more significant in the \gls{dlos} scenario (around $-30\%$ in terms of sum capacity).
The bad performance of \gls{oscbc} compared to \gls{cwc} and \gls{icwc} is confirmed also in terms of fairness, as illustrated in Fig. \ref{fig:quantitelos} (see, in particular, the zoom for $50\leq Z \leq 90$).

Finally, even though \gls{icwc} is not explicitly designed to maximize the sum capacity, it shows similar performance (if not even slightly better) as \gls{cwc} in the \gls{dlos} scenario.
The rationale behind this behavior is not clear and deserves more investigation. Most likely, it is related to the fact that, in the \gls{dlos} scenario, all \glspl{ue} have similar channel conditions, which permits \gls{icwc} to choose, on average, a good \gls{irs} configuration even among the worst \glspl{ue} in the clusters.

\begin{figure}[t]
    \centering
    \setlength\fwidth{0.82\columnwidth}
    \setlength\fheight{0.55\columnwidth}
    \input{Figures/IrsClustering/bars_NIcap.tex}
    %\vspace{-10pt}
    \caption{Average sum capacity for \gls{cwc} and \gls{icwc} as a function of the number of reflecting elements at the \gls{irs}, for unquantized phase shifts, and considering a \gls{plos} channel for the \gls{irs}-\glspl{ue} links.}
    %\vspace{-10pt}
    \label{fig:sumcap_vs_size}
\end{figure}

\begin{figure}[t]
    \centering
    \setlength\fwidth{0.82\columnwidth}
    \setlength\fheight{0.55\columnwidth}
    \input{Figures/IrsClustering/bars_NIperc.tex}
    %\vspace{-10pt}
    \caption{$95$\% percentile of the user capacity for \gls{cwc} and \gls{icwc} as a function of the number of reflecting elements at the \gls{irs}, for unquantized phase shifts, and considering a \gls{plos} channel for the \gls{irs}-\glspl{ue} links.}% The zoom inside the figure is for $N_{\mathrm I} = 200$ and $N_{\mathrm I} = 800$.}
    %\vspace{-10pt}
    \label{fig:quantile_vs_size}
\end{figure}

%\vspace{2pt}\noindent
\emph{Impact of the \gls{irs} configuration.}
Figs. \ref{fig:sumcap_vs_size} and \ref{fig:quantile_vs_size} show the impact of the number of \gls{irs} radiating elements on the system performance when considering the \gls{cwc} and \gls{icwc} clustering algorithms. 
As expected, both fairness (measured in terms of the $95$\% percentile of the average sum capacity) and sum capacity increase as the \gls{irs} is larger and operates with more reflecting elements, regardless of the number of clusters.
For example, we observe that \gls{cwc} is able to approach the optimal sum capacity with as few as $20$ clusters for small-sized \gls{irs}, i.e., with $10$H$\times20$V or $20$H$\times40$V arrays. The same trends are shown also in Fig.~\ref{fig:quantile_vs_size} in terms of fairness.
Still, notice that $\bar{C}$ is below 100~Mbps, which is not compatible with the requirement of most 5G applications when the \gls{irs} is made of fewer than 200 elements, which justifies the use of larger \gls{irs} panels~\cite{pagin2022end}. 

Nevertheless, we still observe that the number of reflecting elements has an impact on the number of clusters that are needed to provide maximum performance. 
Indeed, the number of possible \gls{irs} configurations increases as we consider larger \gls{irs} antennas. In turn, this decreases the likelihood of \glspl{ue} having the same (or similar) ideal configurations, and therefore, it increases the probability of being associated with increasingly suboptimal centroids if the number of clusters is small. However, if the number of phase shifters is large, the suboptimality is mitigated by the increasing number of reconfigurations. Typically, the \gls{irs} reconfiguration cost is proportional to the number of
radiating elements, and therefore the specific reconfiguration cost may be different for different \glspl{irs} in general. A detailed quantitative analysis of this issue would need to go into the specifics of the various \gls{irs} architectures, which goes beyond the scope of the present paper, and will be considered in our future work.

\begin{figure}[t]
    \centering
    \setlength\fwidth{0.82\columnwidth}
    \setlength\fheight{0.55\columnwidth}
    \input{Figures/IrsClustering/cap_quant.tex}
    %\vspace{-10pt}
    \caption{Average sum capacity as a function of the maximum number of clusters   $Z$, for $N_{\mathrm I}=3200$ and for different degrees of quantization of the phase shifts, and considering a \gls{plos} channel for the \gls{irs}-\glspl{ue} links.}
    %\vspace{-10pt}
    \label{fig:cap_vs_bits}
\end{figure}
\begin{figure}[t]
    \centering
    \setlength\fwidth{0.82\columnwidth}
    \setlength\fheight{0.55\columnwidth}
    \input{Figures/IrsClustering/quantile_quant}
    %\vspace{-10pt}
    \caption{$95$\% percentile of the user capacity as a function of the maximum number of clusters   $Z$, for $N_{\mathrm I}=3200$ and for different degrees of quantization of the phase shifts, and considering a \gls{plos} channel for the \gls{irs}-\glspl{ue} links.}
    %\vspace{-10pt}
    \label{fig:quantile_vs_bits}
\end{figure}

%\vspace{2pt}\noindent
\emph{Impact of quantization.} Figs.~\ref{fig:cap_vs_bits} and~\ref{fig:quantile_vs_bits} display the average sum capacity and the $95\%$ percentile, respectively, as a function of the maximum number of clusters $Z$ for \gls{cwc} and \gls{icwc}, and of the number of quantization bits $b$ of the phase shifts. 
Notice that energy and hardware constraints pose a limit to $b$~\cite{rivera2022optimization}, which implies restricting the infinite set of possible \gls{irs} configurations to a finite set of cardinality $2^{bN_{\mathrm I}}$. 
Moreover, the quantization constraint affects the beamforming capabilities of the \gls{irs} \cite{abeywickrama2020intelligent}, with negative implications for the resulting achievable sum capacity.
In \cite{rech2023downlink}, results were obtained considering that the quantization was performed only at the end of the clustering procedure. Here, instead, we assume that the quantization of the phase shifts is taken into account from the initial optimization stage.
The results reveal that the use of non-ideal phase shifters leads to a $30\%$ degradation in the sum capacity when using $b=1$ at the \gls{irs}, while the performance is close to the unquantized baseline if more quantization bits are used.
Furthermore, it is shown that the gap between quantized and the unquantized performance increases with $Z$. As a result, 1-bit quantization is sufficient to guarantee a performance comparable to the unquantized case with a small number of clusters, while more quantization bits are needed to achieve higher capacity.
In any case, we can conclude that our proposed capacity-based clustering algorithms are robust to phase-shift quantization. 

\begin{figure}[t]
    \centering
    \setlength\fwidth{0.82\columnwidth}
    \setlength\fheight{0.55\columnwidth}
    \input{Figures/IrsClustering/sumcap_vsZK}
    %\vspace{-10pt}
    \caption{Sum capacity as a function of the maximum number of clusters over the number of \glspl{ue} $Z/K$, for different values of $K$, for an unquantized $40$H$\times80$V \gls{irs}, $K = \{50, 100, 150\}$, and considering a \gls{plos} channel for the \gls{irs}-\glspl{ue} links. For readability, the results are shown without averaging over the \gls{tdma} frame length.}
    %\vspace{-10pt}
    \label{fig:sumcap_vs_ZK}
\end{figure}

\begin{figure}[t]
    \centering
    \setlength\fwidth{0.82\columnwidth}
    \setlength\fheight{0.55\columnwidth}
    \input{Figures/IrsClustering/quant_vs_ZK}
    %\vspace{-10pt}
    \caption{$95$\% of the user capacity as a function of the maximum number of clusters over the number of \glspl{ue} $Z/K$, for different values of $K$, for an unquantized $40$H$\times80$V \gls{irs}, $K = \{50, 100, 150\}$ \glspl{ue}, and considering a \gls{plos} channel for the \gls{irs}-\glspl{ue} links.}
    %\vspace{-10pt}
    \label{fig:quantcap_vs_ZK}
\end{figure}

\begin{figure}[t]
    \centering
    \setlength\fwidth{0.82\columnwidth}
    \setlength\fheight{0.55\columnwidth}
    \input{Figures/IrsClustering/KvsZ}
    %\vspace{-10pt}
    \caption{Minimum number of \gls{irs} configurations (clusters) $Z_{\mathrm min}$ to achieve 80\% of the maximum achievable sum capacity, for an unquantized $40$H$\times80$V \gls{irs}, and considering a \gls{plos} channel for the \gls{irs}-\glspl{ue} links.}
    %\vspace{-10pt}
    \label{fig:minconfvsN}
\end{figure}

%\vspace{2pt}\noindent
\emph{Scalability.} Finally, we prove the scalability performance of the proposed clustering algorithms. 
To do so, we first show the performance of capacity-based clustering as a function of the number of \glspl{ue} in Figs~\ref{fig:sumcap_vs_ZK} and~\ref{fig:quantcap_vs_ZK}. In particular, we compare \gls{cwc} and \gls{icwc} with \gls{hc} as a function of the ratio $K/N$, for an unquantized $40$H$\times80$V \gls{irs}, $K = \{50, 100, 150\}$ \glspl{ue}, and considering a \gls{plos} channel for the \gls{irs}-\glspl{ue} links.
The results are in line with the plots in Figs~\ref{fig:sumcap0bitlos} and \ref{fig:quantile0bitsplos}, which demonstrates the scalability of the proposed clustering techniques for different numbers of \glspl{ue}.
Finally, Fig.~\ref{fig:minconfvsN} depicts the average minimum number of \gls{irs} configurations  $Z_{\mathrm min}$ needed to achieve 80\% of the maximum achievable sum capacity (``unclustered'' baseline) as a function of the number of \glspl{ue} $K$ in the system.
Notably, we observe that \gls{cwc} and \gls{oscbc} are confirmed to be the best algorithms to optimize the sum capacity, even for a limited number of \gls{irs} configurations. For example, both solutions achieve 80\% of the maximum sum capacity with less than half the number of configurations than in the ``unclustered'' deployment. 
Moreover, we recognize the same trends as in the previous results. Specifically, capacity-based clustering outperforms distance-based clustering and requires fewer \gls{irs} reconfigurations to maximize the sum capacity (up to $-37\%$ considering \gls{cwc} vs. \gls{kmed}). Furthermore, the gap increases as the number of \glspl{ue} increases.

\section{Conclusions}\label{sec:conclusions}
We considered a \gls{mimo} cellular network, in which a \gls{gnb} serving multiple \glspl{ue} is assisted by an \gls{irs} acting as a relay. 
Notably, we considered practical constraints on the \gls{irs} reconfiguration period. We studied a \gls{tdma} scheduling for downlink transmissions, and formulated an optimization problem to maximize the average sum capacity, subject to a fixed number of \gls{irs} reconfigurations per time frame. We first discussed an iterative algorithm to obtain the optimal \gls{irs} configuration of each \gls{ue}. 
Then, we proposed clustering-based scheduling algorithms, which group \glspl{ue} with similar (ideal) \gls{irs} configurations based on either a distance metric or the achievable capacity, to mitigate the performance degradation due to the constraint in the number of possible reconfigurations.
Different clustering algorithms were numerically evaluated in terms of computational complexity, sum capacity, and fairness under different channel conditions, as a function of the size of the \gls{irs} size and the number of users, and with or without quantization of phase shifts.
The results showed that capacity-based clustering outperforms distance-based clustering, and can achieve up to $85$\% of the sum capacity obtained in an ideal deployment (with no reconfiguration constraints), reducing by $50$\% the number of \gls{irs} reconfigurations.

\textcolor{blue}{
\appendix
\section*{Proof of Theorem~\ref{th_distancebased}}
\begin{IEEEproof}[Proof for \gls{km} (LLoyd)]
    In the assignment step, each \gls{ue} $k$ is assigned to the cluster $z$ that minimizes the squared distance $\delta\left(\bm{\Phi}_k^*, \bm{\Phi}^{(z)}\right)^2$. This guarantees that the total sum $J(\mathcal{U}_1,\ldots,\mathcal{U}_{Z},\mathcal{I})$ does not increase. 
    Then, in the update step, $\bm{\Phi}^{(z)}$ is recalculated as the average $\bm{\Phi}_k^*$ within each cluster, so as to minimize the intra-cluster sum of squared distances $\sum_{k \in \mathcal{U}_z}\delta\left(\bm{\Phi}_k^*, \bm{\Phi}^{(z)}\right)^2$, for all $z$.
    Therefore, the conditions of~\cite[Lemma 5]{Sabin1986Global} are satisfied, which ensures the convergence to a local minimum. Notice that~\cite{Sabin1986Global} does not specify the number of iterations needed to reach convergence, which could be large in the case of highly dimensional spaces. Therefore, in practice, we limit the maximum number of iterations to $I_{\mathrm max}^{\mathrm KM}$.
\end{IEEEproof}
\begin{IEEEproof}[Proof for Agglomerative \gls{hc}]
    At each step, clusters are merged to minimize the increase of the total intra-cluster sum of squared distances. This is equivalent to choosing the merged cluster that results in the smallest increase of $J(\mathcal{U}_1,\ldots,\mathcal{U}_{Z},\mathcal{I})$.
    Then, as in the update step of \gls{km}, the average of the data points minimizes $\sum_{k \in \mathcal{U}_z}\delta\left(\bm{\Phi}_k^*, \bm{\Phi}^{(z)}\right)^2$, for all $z$. Once the number of clusters $Z$ reaches the desired value, convergence to a local minimum is reached.
\end{IEEEproof}
\begin{IEEEproof}[Proof for \gls{kmed} (\gls{pam})]
    Since, at each iteration, a swap is performed only when it leads to a lower value of the intra-cluster sum of squares, $J(\mathcal{U}_1,\ldots,\mathcal{U}_{Z},\mathcal{I})$ does not increase over different iterations. Given the finite number of data points and possible configurations, the algorithm is guaranteed to converge to a configuration where no swap can further decrease the objective function, thus reaching a local~minimum.
\end{IEEEproof}
}